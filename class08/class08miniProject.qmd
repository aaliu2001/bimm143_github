---
title: "Class 08 Mini Project"
author: Angela Liu
format: gfm
---

In today's mini-project we will explore a complete analysis using unsupervised learning techniques -- clustering and PCA.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data 

## Data Import

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

Remove the first column:
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

Creating a vector `diagnosis` that has the diagnosis data.
```{r}
# Create diagnosis vector for later 
diagnosis <- wisc.df$diagnosis
diagnosis
```

>Q1. How many observations are in this dataset?

There are 569 observations/patients in the data set.

```{r}
nrow(wisc.data)
```

>Q2. How many of the observations have a malignant diagnosis?

There are 212 observations with a malignant diagnosis. This was found by using `grep()` function to see which elements in `diagnosis` vector have malignant ("M") diagnosis.
```{r}
#grep to look for matches with malignant diagnoses and length to see how many matches in the entire vector
length(grep("M", diagnosis))

#or can use table() function to count up the malignant diagnoses
table(diagnosis)
```


>Q3. How many variables/features in the data are suffixed with _mean?

Let's look at the column names first:
```{r}
colnames(wisc.data)
```

The `grep()` function helps us to look for the column names containing "_mean". The `length()` function will show the results of the vector.
```{r}
#look through column names to find ones with "mean" with the grep function
#use length to find the number of column names with the key word
matches <- (grep("_mean", colnames(wisc.data)))
length(matches)
```
There are 10 variables/features suffixed with _mean.


>Q. How many dimensions are in this dataset?

```{r}
ncol(wisc.data)
```



## Principal Component Analysis (PCA)

Let's check the mean and standard deviation:
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

round(apply(wisc.data,2,sd))
```

The data above is not standardized at all. First we need to scale the data with `prcomp()`. 
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

PC1 covers 44.27% of the original variance of the whole data set. This makes sense because PC1 covers the most and the subsequent PCs start covering less.


>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

We look at the cumulative proportion of the components, and see that three PCs are needed to describe at least 70% of the original variance. Three of them show 72.6% of the original variance.

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

Seven PCs describe at least 90% of the original variance.

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

```{r}
biplot(wisc.pr)
```

This plot is very cluttered and hard to understand. It uses rownames as plotting characters and makes it hard to distinguish any trend with the eye.


Let's make a standard scatter plot of PC1 vs PC2. 
```{r}
#splitting/factoring the diagnosis into groups
diagnosis <- factor(wisc.df$diagnosis)

# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
The separation between the two groups are less distinct than PC1 vs PC2 since they have better proportion of original variance than PC3. 


Use `ggplot` to make a better visualization plot.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=df$diagnosis) + 
  geom_point()

```


## Scree Plots
Scree plots are used to show the proportion of variance. They have 'elbows' where you can figure out number of principal components by eye more naturally. 

We will begin by calculating te variance of each principal component. To do this, we sequare the sdev of `wisc.pr`

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Now we will calculate the variance given by each principal component. We will divide the variance from each PC by the total variance.
```{r}
# Variance explained by each principal component: pve
pve <- pr.var/ sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

How much do the original variables contribute to the new PCs that we have calculated? We can see this through `$rotation`.
```{r}
head(wisc.pr$rotation)
```

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

The component of the loading vector for `concave.points_mean` is -0.26085376. 


There is a complicated mix of variables that go together to make up PC1 - i.e. there are many of the original variables that together contribute highly to PC1.
```{r}
loadings <- as.data.frame(wisc.pr$rotation)

ggplot(loadings) + 
  aes(PC1, rownames(loadings)) +
  geom_col()
```


>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

There are a minimum of 23 PCs required for 80% of the variance.
```{r}
wiscRot.pr <- prcomp(wisc.pr$rotation, scale = TRUE)
summary(wiscRot.pr)
```

## Hierarchical Clustering

Hierarchical clustering computes the distances between all pairs of observations.

We will start by scaling the data. 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```


Calculate the Euclidean distances between all pairs of observations from the scaled data.

```{r}
data.dist <- dist(data.scaled)
```

Now create a hierarchical clustering
```{r}
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
```
 
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The clustering model has 4 clusters at h=19.
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
 
Cut this tree with `cutree()` function.
```{r}
grps <- cutree(wisc.hclust,h=19)
# a table of groups and their diagnoses
table(grps, diagnosis)
```

>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

Yes, we can do this with the `cutree()` function.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=20)
table(wisc.hclust.clusters, diagnosis)
```

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
plot(hclust(data.dist, "single"))
plot(hclust(data.dist, "complete"))
plot(hclust(data.dist, "average"))
plot(hclust(data.dist, "ward.D2"))
```
My favorite is ward.D2 as it shows the clusters better visually and it is easier to identify where the clusters are, especially at the top. It is more spaced out with the clusters.


## Combine Methods: PCA and HCLUST

My PCA results interestingly showed a separation of M and B samples along PC1.
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()`.

Try clustering in 3 PCs, that is PC1, PC2, PC3. 
```{r}
d <- dist(wisc.pr$x[,1:3])

wisc.pr.hclust <- hclust(d, method="ward.D2")
```

Tree result figure:
```{r}
plot(wisc.pr.hclust)
```

Let's cut tthis tree into two groups/clusters
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = grps)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

How well do the clusters separate the M and B diagnoses?
```{r}
table(grps, diagnosis)
```

```{r}
(179+333)/nrow(wisc.data)
```













